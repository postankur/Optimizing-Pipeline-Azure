# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains marketing campaign data for a financial institution. The goal is to predict whether they will subscribe for term deposit. The data contains age, job, marital status, education, housing loan, personal loan, and poutcome which can be used train the model.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was the model with ID HD_fda34223-a94c-456b-8bf7-52e84aa1d17e_14 that was created using hyperdrive parameters and sScikit-learn pipeline. It had an accuracy of **0.91760**. 
The **AutoML model** on the other hand had an accuracy of **0.91618** and the algorithm used was VotingEnsemble.. Th runid was AutoML_ee4a685e-34f2-4031-a4f9-fe96ff33836c_13.


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

**Data**
The data is normalized and cleaned by updating the values in columns(independent variables) from words to binary 0/1 where ever applicable. This helps the the model to run efficiently. The dats is also being split into 80/20 so the model can be tested using predict objects.

**HyperParameter config**
**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling with discrete values is being used. RandomParameterSampling is faster and supports early termination of low-performance runs. GridParameterSampling can be used when budget is not an issue to exhaustively search over the search space or BayesianParameterSampling to explore the hyperparameter space. 

C --> Regularization value. Smaller values have more regularization. max_iter --> Maximum number of iterations.
Below are my config settings:

ps = RandomParameterSampling( 
    {
        "--max_iter": choice(10,50,100,150,200)
        ,"--C": choice(0.001,0.01,0.1,1,1.25,1.5)
    }
)

**What are the benefits of the early stopping policy you chose?**
An early stopping policy is being used to automatically terminate poorly performing runs. This will help improve compute efficiency. Any run that doesn't fall within the slack factor of the evaluation metric with respect to the best performing run will be terminated. 
policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)

hyperdrive_config = HyperDriveConfig(run_config=src,
    hyperparameter_sampling=ps,
    policy=policy,
    primary_metric_name="Accuracy",
    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
    max_total_runs=100,
    max_concurrent_runs=4)

**Classification Algorithm:**
Since we are trying to determine if teh individual will subscribe for short term deposit, i.e. binary outcome, logistic regression is being used as teh classification algorithm.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

Below is the configuration for the AutoML run:

automl_config = AutoMLConfig(
    compute_target=compute_target,
    experiment_timeout_minutes=20,
    task='classification',
    primary_metric="accuracy",
    training_data=train_data,
    label_column_name="y",
    n_cross_validations=3
)

The experiment will continue to run for 20 minutes and exit. Given teh amount of data 20 minute should be more than enough to generate a model.
The primary metric is set as accuracy.
One cross validation can cause an overfit hence it was set to 3. An average of the 3 validation metrics will be used.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Below are the metrics for both runs.

HyperDrive Model
id: HD_fda34223-a94c-456b-8bf7-52e84aa1d17e_14
Accuracy: 0.9176024279210926

AutoML Model
id: AutoML_ee4a685e-34f2-4031-a4f9-fe96ff33836c_13
Accuracy: 0.916176024279211
AUC_weighted: 0.9469939634729121
Algortithm: VotingEnsemble

The difference in accuracy between the two models is rather trivial and although the HyperDrive model performed better in terms of accuracy, I am of the opinion that the AutoML model is actually better because of its **AUC_weighted** metric which equals to **0.9469939634729121** and is more fit for the highly imbalanced data that we have here. If we were given more time to run the AutoML, the resulting model would certainly be much more better. And the best thing is that AutoML would make all the necessary calculations, trainings, validations, etc. without the need for us to do anything. This is the difference with the Scikit-learn Logistic Regression pipeline, in which we have to make any adjustments, changes, etc. by ourselves and come to a final model after many trials & errors. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Cross-validation is the process of taking many subsets of the full training data and training a model on each subset. Higher accuracy can be achieved by higher number in n_cross_validations. I would also increase the experiment_timeout_minutes to help with increased computation required.


## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
